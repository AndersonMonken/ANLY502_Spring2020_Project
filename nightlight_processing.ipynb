{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import fnmatch\n",
    "import json\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import geojsonio\n",
    "from datetime import datetime\n",
    "from shapely.geometry import box,Point, Polygon\n",
    "from shapely.geometry import mapping, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import more packages\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col , column\n",
    "import geopyspark as gps\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = '/mnt/sdb/nightlights/output/'\n",
    "match_string_stable = '*stable_lights.avg_vis.tif'\n",
    "match_string_raw = '*web.avg_vis.tif'\n",
    "match_string_cf = '*web.cf_cvg.tif'\n",
    "def file_list_find(match_string, file_location):\n",
    "    list_files = []\n",
    "    for r,d,f_list in os.walk(file_location):\n",
    "        for f in f_list:\n",
    "            if fnmatch.fnmatch(f,match_string):\n",
    "                list_files.append((r[28:35],os.path.join(r,f)))\n",
    "    \n",
    "    list_files.sort()\n",
    "    \n",
    "    return(list_files)\n",
    "list_files_raw = file_list_find(match_string_raw, file_location)\n",
    "list_files_stable = file_list_find(match_string_stable, file_location)\n",
    "list_files_cf = file_list_find(match_string_cf, file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ADMIN ISO_A3                                           geometry\n",
      "0        Aruba    ABW  POLYGON ((-69.99694 12.57758, -69.93639 12.531...\n",
      "1  Afghanistan    AFG  POLYGON ((71.04980 38.40866, 71.05714 38.40903...\n",
      "2       Angola    AGO  MULTIPOLYGON (((11.73752 -16.69258, 11.73851 -...\n",
      "3     Anguilla    AIA  MULTIPOLYGON (((-63.03767 18.21296, -63.09952 ...\n",
      "4      Albania    ALB  POLYGON ((19.74777 42.57890, 19.74601 42.57993...\n"
     ]
    }
   ],
   "source": [
    "countries = gpd.read_file('countries.geojson')\n",
    "print(countries.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SparkContext\n",
    "conf = gps.geopyspark_conf(appName=\"geopyspark-example\", master=\"local[*]\")\n",
    "conf.set(\"spark.executor.memory\", '13g')\n",
    "conf.set('spark.executor.cores', '4')\n",
    "conf.set('spark.cores.max', '24')\n",
    "conf.set(\"spark.driver.memory\",'14g')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.kryo.registrator',\n",
       "  'geopyspark.geotools.kryo.ExpandedKryoRegistrator'),\n",
       " ('spark.app.id', 'local-1586796903124'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.local.dir', '/mnt/nvme0n1p5/tmp'),\n",
       " ('spark.memory.offHeap.enabled', 'true'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.driver.maxResultSize', '20g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.ui.enabled', 'false'),\n",
       " ('spark.driver.port', '35431'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.memory', '13g'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///opt/anaconda3/lib/python3.7/site-packages/geopyspark/jars/geotrellis-backend-assembly-0.4.3.jar'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '192.168.0.137'),\n",
       " ('spark.cores.max', '24'),\n",
       " ('spark.app.name', 'geopyspark-example'),\n",
       " ('spark.driver.memory', '14g'),\n",
       " ('spark.jars',\n",
       "  '/opt/anaconda3/lib/python3.7/site-packages/geopyspark/jars/geotrellis-backend-assembly-0.4.3.jar'),\n",
       " ('spark.driver.cores', '22'),\n",
       " ('spark.memory.offHeap.size', '16g'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a few files\n",
    "list_tile = []\n",
    "years = []\n",
    "Image.MAX_IMAGE_PIXELS = 933120000\n",
    "for cam, im_file in list_files_stable[0:1]:\n",
    "    im = Image.open(im_file)\n",
    "    imarray=np.array(im)\n",
    "    list_tile.append(gps.Tile.from_numpy_array(numpy_array=imarray, no_data_value=255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to choose the correct epsg code, which determines the geometry of the data. \n",
    "    See Harvard's view of this same data using the epsg 4326 format.\n",
    "    http://worldmap.harvard.edu/data/geonode:global_nightlights_KpX\n",
    "    \n",
    "More info on this geography system can be found here - https://epsg.io/4326 \n",
    "\n",
    "But this is actually not the epsg format that most maps use. We actually need 3857 if we want to plot the data on a world map that the public is used to seeing https://epsg.io/3857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = gps.Extent(-65.0,-180.0, 75.0, 180.0)\n",
    "extent\n",
    "projected_extent = gps.ProjectedExtent(extent=extent, epsg=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are server limitations to the maximum amount of memory available in an individual JVM. The Java heap space must be large enough to deal with the large amounts of memory that we're working with. Unfortunately, even with increasing the executor memory on each worker and trying out different number of cores per worker, combining multiple images together led to Java heap space errors. This happened when there were 6 cores and 20gb of memory per worker. The workaround is to process each image individually due to this potential memory leak error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd = sc.parallelize([(gps.TemporalProjectedExtent(extent=extent,instant = datetime.now(), proj4='+proj=longlat +datum=WGS84 +no_defs'), x) for x in list_tile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd = sc.parallelize([(gps.ProjectedExtent(extent=extent, proj4='+proj=longlat +datum=WGS84 +no_defs'), x) for x in list_tile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiband_raster_layer = gps.RasterLayer.from_numpy_rdd(layer_type=gps.LayerType.SPATIAL, numpy_rdd=rdd)\n",
    "#multiband_raster_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiledRaster_layer = multiband_raster_layer.tile_to_layout(layout=gps.LocalLayout())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with F101992 in 0:06:27.145966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with F101993 in 0:12:49.401312\n",
      "Done with F101994 in 0:19:07.701878\n",
      "Done with F121994 in 0:25:25.263820\n",
      "Done with F121995 in 0:31:43.764016\n",
      "Done with F121996 in 0:37:59.883265\n",
      "Done with F121997 in 0:44:15.901093\n",
      "Done with F121998 in 0:50:34.886535\n",
      "Done with F121999 in 0:56:53.154173\n",
      "Done with F141997 in 1:03:09.765484\n",
      "Done with F141998 in 1:09:27.610768\n",
      "Done with F141999 in 1:15:44.604656\n",
      "Done with F142000 in 1:22:01.475432\n",
      "Done with F142001 in 1:28:15.239757\n",
      "Done with F142002 in 1:34:31.898589\n",
      "Done with F142003 in 1:40:54.958446\n",
      "Done with F152000 in 1:47:18.700362\n",
      "Done with F152001 in 1:53:38.370945\n",
      "Done with F152002 in 2:00:00.861731\n",
      "Done with F152003 in 2:06:22.880606\n",
      "Done with F152004 in 2:12:42.384670\n",
      "Done with F152005 in 2:19:08.309321\n",
      "Done with F152006 in 2:25:28.357693\n",
      "Done with F152007 in 2:31:46.020877\n",
      "Done with F162004 in 2:38:05.793990\n",
      "Done with F162005 in 2:44:24.228454\n",
      "Done with F162006 in 2:50:43.503134\n",
      "Done with F162007 in 2:57:03.877921\n",
      "Done with F162008 in 3:03:25.629873\n",
      "Done with F162009 in 3:09:45.359343\n",
      "Done with F182010 in 3:16:03.150574\n",
      "Done with F182011 in 3:22:22.311842\n",
      "Done with F182012 in 3:28:40.829564\n",
      "Done with F182013 in 3:34:59.768984\n"
     ]
    }
   ],
   "source": [
    "# Main loop and code\n",
    "t1 = datetime.now()\n",
    "list_tile = []\n",
    "df_lights = pd.DataFrame()\n",
    "for cam, im_file in list_files_stable:\n",
    "    countries_i = countries.copy()\n",
    "    raster_layer = gps.geotiff.get(layer_type=gps.LayerType.SPATIAL, uri=im_file)\n",
    "    tiledRaster_layer = raster_layer.tile_to_layout(layout=gps.LocalLayout())\n",
    "    \n",
    "    countries['mean_light'] = None\n",
    "    for i in range(0,len(countries)):\n",
    "        country_shape_i = countries.loc[i,'geometry']\n",
    "        light_i = tiledRaster_layer.polygonal_mean(country_shape_i)\n",
    "        countries.loc[i, 'mean_light'] = light_i\n",
    "        #print(f\"{countries.loc[i,'ADMIN']}:{light_i}\")\n",
    "        \n",
    "    countries_i = countries_i.drop('geometry', axis = 1)\n",
    "    countries_i['year'] = int(cam[-4:])\n",
    "    countries_i['cam'] = cam\n",
    "    df_lights = df_lights.append(countries_i)\n",
    "    t2 = datetime.now()\n",
    "    print(f\"Done with {cam} in {t2-t1}\")\n",
    "t3 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished in: 3:34:59.769137\n"
     ]
    }
   ],
   "source": [
    "print(f'Program finished in: {t3-t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8670"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_lights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lights_pd = pd.DataFrame(df_lights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8250.000000\n",
       "mean        6.720403\n",
       "std        11.564157\n",
       "min         0.000000\n",
       "25%         0.255506\n",
       "50%         1.680038\n",
       "75%         7.622372\n",
       "max        62.947899\n",
       "Name: mean_light, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lights_pd['mean_light'].astype('float').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 420\n"
     ]
    }
   ],
   "source": [
    "print(f'Missing values: {str(sum(df_lights_pd.mean_light.isna()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lights_pd.to_csv('lights_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_lights_pd,open('lights_data.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
